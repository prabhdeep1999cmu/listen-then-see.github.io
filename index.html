<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Cross-modal alignment and representation fusion approach for Socially Intelligent Question Answering, achieving a SOTA 82.06% accuracy on the Social IQ 2.0 dataset by enhancing video modality utilization and mitigating language overfitting.">
  <meta name="keywords" content="Listen Then See, Multimodal, CVPR, MULA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Listen Then See: Video Alignment with Speaker Attention</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="TODO">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Listen Then See: Video Alignment with Speaker Attention</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://aviral-agrawal.github.io">Aviral Agrawal*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/carlos-mateo-samudio-lezcano-a399a5272">Carlos Mateo Samudio Lezcano*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://iquibalamhm.github.io">Iqui Balam Heredia-Marin*</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://prabhdeep1999.github.io/Prabhdeep1999/">Prabhdeep Singh Sethi*</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Carnegie Mellon University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024W/MULA/papers/Agrawal_Listen_Then_See_Video_Alignment_with_Speaker_Attention_CVPRW_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2404.13530"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/sts-vlcc/sts-vlcc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Slides Link. -->
              <span class="link-block">
                <a href="https://docs.google.com/presentation/d/1bMC-n0HvY8B1zgPTkaE-3Ku2pQqbylinxpQw8Crd2hM/edit?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/final_architecture_image.png"
                 class="architecture-image"
                 alt="Architecture Diagram"/>
      <h2 class="subtitle has-text-centered"> -->
        <!-- <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits. -->
      <!-- </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video-based Question Answering (Video QA) is a challenging task 
            and becomes even more intricate when addressing Socially 
            Intelligent Question Answering (SIQA). SIQA requires context 
            understanding, temporal reasoning, and the integration of
            multimodal information, but in addition, it requires processing 
            nuanced human behavior. Furthermore, the complexities involved 
            are exacerbated by the dominance of the primary modality (text) 
            over the others. Thus, there is a need to help the task's 
            secondary modalities to work in tandem with the primary 
            modality. In this work, we introduce a cross-modal alignment 
            and subsequent representation fusion approach that achieves 
            state-of-the-art results (82.06\% accuracy) on the Social 
            IQ 2.0 dataset for SIQA. Our approach exhibits an improved 
            ability to leverage the video modality by using the audio 
            modality as a bridge with the language modality. This leads 
            to enhanced performance by reducing the prevalent issue of 
            language overfitting and resultant video modality bypassing 
            encountered by current existing techniques.  Our code and models 
            are publicly available at: https://github.com/sts-vlcc/sts-vlcc
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper Image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Architecture</h2>
        <div class="hero-body">
          <img src="./static/images/final_architecture_image.png"
                 class="final-architecture-image"
                 alt="final_architecture_image"/>
                 <h2 class="content has-text-justified"> 
                   The figure displays the proposed architecture. We run the Speaking 
                   Turn Sampling (STS) module to the aligned frame_i from 
                   the speaking turn k and the corresponding subtitle from 
                   the transcript. We pass this pair to the frozen CLIP encoder to 
                   obtain the visual and text encodings respectively. The resultant 
                   encodings are passed through the Vision Language Cross 
                   Contextualization (VLCC) module and subsequently passed through the 
                   projection layer to generate one of the inputs to the LLM. 
                   Simultaneously, we generate the text embeddings of size U 
                   for each question-answer pair, and the text embeddings of size 
                   V for the video subtitles.
                 </h2>
        </div>
      </div>
    </div>
    <!--/ Paper Image. -->

    <!-- Dataset -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
        <div class="hero-body">
          <img src="./static/images/banner.png"
                 class="final-architecture-image"
                 alt="final_architecture_image"/>
                 <h2 class="content has-text-justified"> 
                  <p>
                  We use the SocialIQ 2.0 dataset which follows the guidelines
                  for measuring social intelligence. This dataset consists of 
                  1,400 social in-the-wild videos annotated with 8,076 
                  questions and 32,304 answers (4 answers per question, 
                  3 incorrect, 1 correct). We use this dataset which is the 
                  only dataset that captures social intelligence in the VQA 
                  setup. 
                  </p>
                  <p>
                  The dataset, includes videos (mp4), audio (mp3, wav) and 
                  transcripts (vtt). 
                  </p>
                </h2>
        </div>
      </div>
    </div>
    <!--/ Dataset -->

    <!-- Speaking Turn -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Speaking Turn</h2>
        <div class="hero-body">
          <img src="./static/images/fin_img_sampling_res.png"
                 class="final-architecture-image"
                 alt="final_architecture_image"/>
                 <h2 class="content has-text-justified"> 
                  This is a demonstration of our Speaking Turn Sampling: 
                  The question asked in this video is "What is the tone of 
                  the people speaking?". This example shows that our method 
                  (in the green box) uses more relevant frames where people 
                  are speaking. In contrast, the baseline (in the red box) 
                  samples frames that do not contain relevant information 
                  for the task. In this example, our model predicts the 
                  correct answer, whereas the baseline does not.
                </h2>
        </div>
      </div>
    </div>
    <!--/ Dataset -->
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{agrawal2024listen,
      title={Listen Then See: Video Alignment with Speaker Attention},
      author={Agrawal, Aviral and Lezcano, Carlos Mateo Samudio and Heredia-Marin, Iqui Balam and Sethi, Prabhdeep Singh},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      pages={2018--2027},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
